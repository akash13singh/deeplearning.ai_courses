{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227fc92c-3f15-4ffb-b921-b5b346e4a79f",
   "metadata": {},
   "source": [
    "## L1 Language Models, the Chat Format and Tokens\n",
    "### Course Deeplearning.ai: Building Systems with the ChatGPT API\n",
    "### URL: https://learn.deeplearning.ai/chatgpt-building-system/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a13f2bf-80d4-4f53-a312-4b9af33d5f99",
   "metadata": {},
   "source": [
    "### Flow\n",
    "#### We need to chain multiple calls to a language model, using different instructions depending on the output of the previous call, and sometimes even looking things up from external sources. \n",
    "* Evaluate input to ensure there is no problematic content like hate speech\n",
    "* Infer query type/intent: complaint, product info request etc.\n",
    "* Extract relevant info\n",
    "* Write helpful output\n",
    "* Check if output is not problematic, innaccurate, or inappropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767e535-65d8-40bc-90cc-75a4286ec01a",
   "metadata": {},
   "source": [
    "### Two types of LLM\n",
    "#### **Base LLM**: \n",
    "- works by iteratively predicting next word\n",
    "- e.g. GPT\n",
    "#### **Instruction tuned LLM**\n",
    "- trained to answer instructions\n",
    "- e.g. ChatGPT\n",
    "\n",
    "#### Training process.\n",
    "- First the Base LLM is trained on lots of data (100 of billions of words) for months\n",
    "- The Base LLM is then finetuned on a smaller dataset which consists of instructions and answers. This dataset is often mannually created.\n",
    "- Once finetuned, the models output are rated by humans on different criteria: correctness, harmful, helpful etc.\n",
    "- Then the LLM is futher fine tuned to increase the probability of generating the correct output. The common alogrithm for this is RLHF: reinforcement Learning from Human Feedback (RLHF)\n",
    "- Training an instruction tuned LLM fgrom base LLM requires modest tiem and data compared to the base LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4caf71d-b9e5-440f-8792-b26e938f8b62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7a53e38-3305-45f7-9972-b1583ed505c1",
   "metadata": {},
   "source": [
    "### Setup\n",
    "#### Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fd7b8b-0145-4309-916d-295cfc0963ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "import tokenizers\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d28e5f-6fb4-4634-8c65-7e65aa10e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592fc496-f8b5-4ea1-8ccb-9a4270f2e4af",
   "metadata": {},
   "source": [
    "#### OpenAI helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5801426-0703-4939-8398-d501064d77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bcdc88-a8ae-49ad-8898-a5ad310e748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af92171-fb49-4240-8b07-f208ce5acf59",
   "metadata": {},
   "source": [
    "#### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46493006-88e6-4e6c-a9dc-711bcf18ea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reversed letters of \"lollipop\" are \"pillipol\".\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a719668-4723-46ff-ad7f-954604cba078",
   "metadata": {},
   "source": [
    "#### \"lollipop\" in reverse should be \"popillol\"\n",
    "##### But Chat GPT fails, because it works on tokens\n",
    "##### Instead if we add - between letters, each letter is a different token and model is able to reverse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f29e2b5-d18f-4517-a490-b281bc1308d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-o-p-i-l-l-o-l\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f57612-3f19-4f9e-8b10-589ef3e38b89",
   "metadata": {},
   "source": [
    "#### A note about the backslash\n",
    "- In the course, we are using a backslash `\\` to make the text fit on the screen without inserting newline '\\n' characters.\n",
    "- GPT-3 isn't really affected whether you insert newline characters or not.  But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b329b7ba-04f0-4a31-a1cf-953d1685496f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get encoding scheme\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02cf5eb8-3039-4a96-8a82-6294251635ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75, 90644]\n"
     ]
    }
   ],
   "source": [
    "### print tokens\n",
    "print(encoding.encode('lollipop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90582ec2-a165-42e8-ad06-582185ec4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75, 16405, 2922, 2922, 18064, 2320, 16405, 2320]\n"
     ]
    }
   ],
   "source": [
    "print(encoding.encode('l-o-l-l-i-p-o-p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc573fee-4d66-4e08-9682-9d46a2bc25a5",
   "metadata": {},
   "source": [
    "### 1 token is 4 characters or 3/4th of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76efc51-f45a-4e2d-a7a2-7974a2720558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l-o-l-l-i-p-o-p'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([75, 16405, 2922, 2922, 18064, 2320, 16405, 2320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d4e9d67-7477-4e50-8874-723b65a53200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lollipop'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([75, 90644])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75634fe1-b30c-46d9-840d-9e12fbca7bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'l', b'-o', b'-l', b'-l', b'-i', b'-p', b'-o', b'-p']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode_tokens_bytes([75, 16405, 2922, 2922, 18064, 2320, 16405, 2320])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a0ec78a-c741-4f5c-b8a2-57301d645593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'l', b'ollipop']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode_tokens_bytes([75, 90644])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b8522a-ee41-448e-8704-03261b12faf4",
   "metadata": {},
   "source": [
    "## Helper function (chat format)\n",
    "Here's the helper function we'll use in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ab8ac88-ef9a-4b41-8315-4e63dc105619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b45d629d-73f1-4ce5-8a47-fe3b4e7d2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, a happy carrot grew,\n",
      "Beneath the golden sun, its vibrant hue.\n",
      "With leaves so green, and a smile so wide,\n",
      "This little carrot brought joy far and wide.\n",
      "\n",
      "It danced in the breeze, oh what a sight,\n",
      "Its orange body shining, pure delight.\n",
      "From garden to table, its journey did go,\n",
      "Spreading happiness, row after row.\n",
      "\n",
      "With each crunchy bite, the joy did spread,\n",
      "From toe to tummy, it filled with widespread.\n",
      "A happy carrot, oh so grand,\n",
      "Brings happiness to all, throughout the land.\n",
      "\n",
      "So remember, my friend, when feeling blue,\n",
      "Just think of the carrot, so joyful and true.\n",
      "May its happiness fill your every day,\n",
      "And keep your worries, far, far away!\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5648abeb-4aea-4900-950c-f07acd246b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a cheerful carrot named Charlie who brightened everyone's day with his vibrant orange hue and infectious laughter.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dac8c66-9cbe-4b09-8dcb-c5eaff2e59c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a garden so grand, there lived a happy carrot with a smile so grand.\n"
     ]
    }
   ],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who responds in the style of Dr Seuss. All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7d3faef-e58f-4625-9fff-a95badb33c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-3.5-turbo\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message[\"content\"]\n",
    "    \n",
    "    token_dict = {\n",
    "'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "'completion_tokens':response['usage']['completion_tokens'],\n",
    "'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f62f6e71-7f50-460f-a598-419d4b82e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\ \n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f3ef26d-434f-4a17-aa28-6e9edca3711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the happy carrot, so bright and orange,\n",
      "Grown in the garden, a joyful forage.\n",
      "With a smile so wide, from top to bottom,\n",
      "It brings happiness, oh how it blossoms!\n",
      "\n",
      "In the soil it grew, with love and care,\n",
      "Nourished by sunshine, fresh air to share.\n",
      "Its leaves so green, waving in the breeze,\n",
      "A happy carrot, it aims to please.\n",
      "\n",
      "With a crunch and a munch, it brings delight,\n",
      "A healthy snack, oh what a sight!\n",
      "From garden to plate, it brings such glee,\n",
      "The happy carrot, a friend to me.\n",
      "\n",
      "So let's celebrate this veggie so grand,\n",
      "With a joyful dance, let's all take a stand.\n",
      "For the happy carrot, so full of cheer,\n",
      "A simple pleasure, oh how it's dear!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccab517a-eaea-487f-ae47-9af784acb528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 37, 'completion_tokens': 166, 'total_tokens': 203}\n"
     ]
    }
   ],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ba758-8a3f-4872-bd9e-b70b9296d844",
   "metadata": {},
   "source": [
    "## Notes on using the OpenAI API outside of this classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c302b16-6ac9-4d1d-adb0-2e97086b5d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
